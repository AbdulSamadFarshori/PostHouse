{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b8bc60e-08f2-4f3f-a757-16d2e9959fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Optional, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "class ScrapingEngine:\n",
    "    \"\"\"\n",
    "    A web scraping engine using Selenium to fetch HTML and BeautifulSoup for parsing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        headless: bool = True,\n",
    "        driver_path: str = \"../drive/chromedriver.exe\",\n",
    "        urls: Optional[List[str]] = None,\n",
    "        tag: str = \"p\",\n",
    "        max_retries: int = 3,\n",
    "        scroll_count: int = 3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the scraping engine.\n",
    "\n",
    "        :param headless: Run Chrome in headless mode.\n",
    "        :param driver_path: Path to ChromeDriver.\n",
    "        :param urls: List of URLs to scrape.\n",
    "        :param max_retries: Number of retries on failure.\n",
    "        :param scroll_count: Number of scrolls for dynamic content.\n",
    "        \"\"\"\n",
    "        options = Options()\n",
    "        options.headless = headless\n",
    "        self.driver = webdriver.Chrome(service=Service(driver_path), options=options)\n",
    "        self.urls = urls or []\n",
    "        self.tag: str = tag\n",
    "        self.max_retries = max_retries\n",
    "        self.scroll_count = scroll_count\n",
    "        self.page_soups: Dict[str, BeautifulSoup] = {}\n",
    "        self.cleaned_texts: Dict[str, BeautifulSoup] = {}\n",
    "        self._all_paragraph_text: List[str] = []\n",
    "\n",
    "    def engine(self, url: str):\n",
    "        retry = 0\n",
    "        success = False\n",
    "        while retry < self.max_retries and not success:\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                time.sleep(3)\n",
    "\n",
    "                for _ in range(self.scroll_count):\n",
    "                    self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                html = self.driver.page_source\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                print(f\"[SUCCESS] Scraped: {url}\")\n",
    "                success = True\n",
    "                self.driver.quit()\n",
    "                return soup\n",
    "            except Exception as e:\n",
    "                retry += 1\n",
    "                wait_time = 2 ** retry\n",
    "                print(f\"[RETRY {retry}/{self.max_retries}] Error scraping {url}: {e}. Retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "        self.driver.quit()\n",
    "\n",
    "    \n",
    "    def fetch_and_parse(self):\n",
    "        \"\"\"\n",
    "        Fetch HTML content from URLs and parse it using BeautifulSoup.\n",
    "        Stores the parsed soup object in self.page_soups.\n",
    "        \"\"\"\n",
    "        for url in self.urls:\n",
    "            HTML_object = self.engine(url)\n",
    "            self.page_soups[url] = HTML_object\n",
    "            \n",
    "\n",
    "    def fliter_soup(self):\n",
    "        for soup in self.page_soups.values():\n",
    "            for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "                tag.decompose()\n",
    "            for tag in soup.find_all([\"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "                tag.decompose()\n",
    "            keywords = [\"navbar\", \"nav\", \"footer\", \"header\", \"menu\", \"sidebar\"]\n",
    "            for keyword in keywords:\n",
    "                for tag in soup.find_all(attrs={\"class\": lambda x: x and keyword in x.lower()}):\n",
    "                    tag.decompose()\n",
    "                for tag in soup.find_all(attrs={\"id\": lambda x: x and keyword in x.lower()}):\n",
    "                    tag.decompose()\n",
    "            \n",
    "            clean_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def get_all_p_text(self):\n",
    "        self.fetch_and_parse()\n",
    "        for soup in self.page_soups.values(): \n",
    "            paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n",
    "            self._all_paragraph_text.append(\"\\n\\n\".join(paragraphs))\n",
    "        return self._all_paragraph_text        \n",
    "\n",
    "    def invoke(self):\n",
    "        if self.tag == \"p\":\n",
    "            paragraphs = self.get_all_p_text()\n",
    "            content = \"\\n\\n\".join(paragraphs)\n",
    "            return content\n",
    "        \n",
    "\n",
    "class CompetitorWebsiteScraper(ScrapingEngine):\n",
    "\n",
    "    def __init__(self, url: str):\n",
    "        super().__init__()\n",
    "        self.url = url\n",
    "        self.HTML = None\n",
    "\n",
    "    def fetch_soup_object(self):\n",
    "        self.HTML = self.engine(self.url)\n",
    "        return self.HTML\n",
    "    \n",
    "    def get_title(self):\n",
    "        # self.fetch_soup_object()\n",
    "        title = self.HTML.title.text if self.HTML.title else \"No title found\"\n",
    "        return title\n",
    "\n",
    "    def get_all_links(self):\n",
    "        # self.fetch_soup_object()\n",
    "        body = self.HTML.find('body')\n",
    "        links = [a['href'] for a in body.find_all('a', href=True)] if body else []\n",
    "        return links\n",
    "\n",
    "    def get_all_keywords(self):\n",
    "        keywords_tag = self.HTML.find('meta', attrs={'name': 'keywords'})\n",
    "        keywords = keywords_tag['content'] if keywords_tag else \"No keywords found\"\n",
    "        return keywords\n",
    "    \n",
    "    def get_description(self):\n",
    "        description = self.HTML.find('meta', attrs={'name': 'description'})\n",
    "        if description:\n",
    "            description_content = description.get('content')\n",
    "            return description_content \n",
    "        else:\n",
    "            description_content = 'No description found'\n",
    "            return description_content\n",
    "\n",
    "    def get_robots(self):\n",
    "        robots = self.HTML.find('meta', attrs={'name': 'robots'})\n",
    "        if robots:\n",
    "            robots_content =  robots.get('content')\n",
    "            return robots_content\n",
    "        else:\n",
    "            robots_content =  \"No robots found\"\n",
    "            return robots_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f572de3-e83f-4e2f-bcb6-7ef5c19bcd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RETRY 1/3] Error scraping h: Message: invalid argument\n",
      "  (Session info: chrome=138.0.7204.101)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6c4016f95+76917]\n",
      "\tGetHandleVerifier [0x0x7ff6c4016ff0+77008]\n",
      "\t(No symbol) [0x0x7ff6c3dc9c1c]\n",
      "\t(No symbol) [0x0x7ff6c3db6c88]\n",
      "\t(No symbol) [0x0x7ff6c3db503c]\n",
      "\t(No symbol) [0x0x7ff6c3db570c]\n",
      "\t(No symbol) [0x0x7ff6c3dcdd9a]\n",
      "\t(No symbol) [0x0x7ff6c3e7143e]\n",
      "\t(No symbol) [0x0x7ff6c3e4846a]\n",
      "\t(No symbol) [0x0x7ff6c3e7065c]\n",
      "\t(No symbol) [0x0x7ff6c3e48243]\n",
      "\t(No symbol) [0x0x7ff6c3e11431]\n",
      "\t(No symbol) [0x0x7ff6c3e121c3]\n",
      "\tGetHandleVerifier [0x0x7ff6c42ed2cd+3051437]\n",
      "\tGetHandleVerifier [0x0x7ff6c42e7923+3028483]\n",
      "\tGetHandleVerifier [0x0x7ff6c43058bd+3151261]\n",
      "\tGetHandleVerifier [0x0x7ff6c403185e+185662]\n",
      "\tGetHandleVerifier [0x0x7ff6c403971f+218111]\n",
      "\tGetHandleVerifier [0x0x7ff6c401fb14+112628]\n",
      "\tGetHandleVerifier [0x0x7ff6c401fcc9+113065]\n",
      "\tGetHandleVerifier [0x0x7ff6c4006c98+10616]\n",
      "\tBaseThreadInitThunk [0x0x7ffb9e3be8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffba005c34c+44]\n",
      ". Retrying in 2s...\n",
      "[RETRY 2/3] Error scraping h: Message: invalid argument\n",
      "  (Session info: chrome=138.0.7204.101)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6c4016f95+76917]\n",
      "\tGetHandleVerifier [0x0x7ff6c4016ff0+77008]\n",
      "\t(No symbol) [0x0x7ff6c3dc9c1c]\n",
      "\t(No symbol) [0x0x7ff6c3db6c88]\n",
      "\t(No symbol) [0x0x7ff6c3db503c]\n",
      "\t(No symbol) [0x0x7ff6c3db570c]\n",
      "\t(No symbol) [0x0x7ff6c3dcdd9a]\n",
      "\t(No symbol) [0x0x7ff6c3e7143e]\n",
      "\t(No symbol) [0x0x7ff6c3e4846a]\n",
      "\t(No symbol) [0x0x7ff6c3e7065c]\n",
      "\t(No symbol) [0x0x7ff6c3e48243]\n",
      "\t(No symbol) [0x0x7ff6c3e11431]\n",
      "\t(No symbol) [0x0x7ff6c3e121c3]\n",
      "\tGetHandleVerifier [0x0x7ff6c42ed2cd+3051437]\n",
      "\tGetHandleVerifier [0x0x7ff6c42e7923+3028483]\n",
      "\tGetHandleVerifier [0x0x7ff6c43058bd+3151261]\n",
      "\tGetHandleVerifier [0x0x7ff6c403185e+185662]\n",
      "\tGetHandleVerifier [0x0x7ff6c403971f+218111]\n",
      "\tGetHandleVerifier [0x0x7ff6c401fb14+112628]\n",
      "\tGetHandleVerifier [0x0x7ff6c401fcc9+113065]\n",
      "\tGetHandleVerifier [0x0x7ff6c4006c98+10616]\n",
      "\tBaseThreadInitThunk [0x0x7ffb9e3be8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffba005c34c+44]\n",
      ". Retrying in 4s...\n",
      "[RETRY 3/3] Error scraping h: Message: invalid argument\n",
      "  (Session info: chrome=138.0.7204.101)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6c4016f95+76917]\n",
      "\tGetHandleVerifier [0x0x7ff6c4016ff0+77008]\n",
      "\t(No symbol) [0x0x7ff6c3dc9c1c]\n",
      "\t(No symbol) [0x0x7ff6c3db6c88]\n",
      "\t(No symbol) [0x0x7ff6c3db503c]\n",
      "\t(No symbol) [0x0x7ff6c3db570c]\n",
      "\t(No symbol) [0x0x7ff6c3dcdd9a]\n",
      "\t(No symbol) [0x0x7ff6c3e7143e]\n",
      "\t(No symbol) [0x0x7ff6c3e4846a]\n",
      "\t(No symbol) [0x0x7ff6c3e7065c]\n",
      "\t(No symbol) [0x0x7ff6c3e48243]\n",
      "\t(No symbol) [0x0x7ff6c3e11431]\n",
      "\t(No symbol) [0x0x7ff6c3e121c3]\n",
      "\tGetHandleVerifier [0x0x7ff6c42ed2cd+3051437]\n",
      "\tGetHandleVerifier [0x0x7ff6c42e7923+3028483]\n",
      "\tGetHandleVerifier [0x0x7ff6c43058bd+3151261]\n",
      "\tGetHandleVerifier [0x0x7ff6c403185e+185662]\n",
      "\tGetHandleVerifier [0x0x7ff6c403971f+218111]\n",
      "\tGetHandleVerifier [0x0x7ff6c401fb14+112628]\n",
      "\tGetHandleVerifier [0x0x7ff6c401fcc9+113065]\n",
      "\tGetHandleVerifier [0x0x7ff6c4006c98+10616]\n",
      "\tBaseThreadInitThunk [0x0x7ffb9e3be8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffba005c34c+44]\n",
      ". Retrying in 8s...\n",
      "[RETRY 1/3] Error scraping t: HTTPConnectionPool(host='localhost', port=49188): Max retries exceeded with url: /session/4e591342ee97a445d68e3edb21b747f3/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000018D45F6CD10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')). Retrying in 2s...\n"
     ]
    }
   ],
   "source": [
    "domain = \"https://nesolarandgreen.com/\"\n",
    "url = f\"https://www.similarweb.com/website/{domain}/\"\n",
    "\n",
    "s = ScrapingEngine(urls=[url])\n",
    "s.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8681b4-2fcf-44bd-b7f9-877827e44ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
